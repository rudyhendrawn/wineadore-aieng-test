# Wine Adore AI Engineer Take Home Test

## Project Purpose
Design and implement an end-to-end machine learning solution for solving business problem.

## Business Problem
Convert users who have not yet made purchases (non-consumers ‚Üí consumers).

> **Note**
> - Consumer: users who have made at least one purchase
> - Non-consumer: users who have not made any purchases

## Data Collection
[Fridrich, M. (2023). Machine Learning in Customer Churn Prediction [Dissertation Thesis]. Brno University of Technology, Faculty of Business and Management. Supervised by Petr Dost√°l.](https://www.kaggle.com/datasets/fridrichmrtn/e-commerce-churn-dataset-rees46/data)

### Business Metrics
- **Revenue**: Average **`target_actual_profit`**. Revenue (or profit contribution) is the most direct measurement of customer value.
- **Transactions**: Average **`transaction_count`**. The number of transactions is a direct indicator of how consistently a user converts.
- **Sessions**: Average **`session_count`**. The number of sessions indicates user engagement and interest in the platform.
- **Recency**: Average **`recency`** (days since last activity). A lower recency indicates more recent activity, which is often correlated with higher conversion likelihood.
- **Category Diversity**: Average **`category_diversity`**. The variety of product categories a user interacts with can indicate broader interests and potential for cross-selling.

### Why this dataset?
This dataset contains user behavior data from an e-commerce platform, including various features that can help predict whether a user is likely to make a purchase or not. It is similar to the business problem at Wine Adore, where we want to identify users who are likely to convert into consumers.

## Purpose of the project
- Simulate Customer Segmentation: Profile customer segments based on their behavior.
- Simulate "Users->Consumers" Conversion: Identify patterns and features that differentiate users who make purchases from those who do not.

## Project Flow and How to Replicate
1. Download data from [Kaggle](https://www.kaggle.com/datasets/fridrichmrtn/e-commerce-churn-dataset-rees46/data?select=rees46_customer_model.csv)
2. Copy the CSV to `datasets` directory
3. Run CSV to Parquet conversion script to convert large CSV file to parquet for optimum reading large file dataset.
   ```
   python ./scripts/csv_to_parquet.py
   ```
4. We ran [EDA](./notebooks/eda.ipynb) to determined the possible methodology for clustering experimentation.
5. Ran [experiment](./notebooks/segmentation_experiment.ipynb) to find best clusters and segmentation, then analyze the results based on business objective.

---

## Experiment Methodology

### 1. Grid Search Approach
We conducted a comprehensive grid search experiment across 144 parameter combinations to find optimal clustering configurations:

**Parameter Grid:**
- **Correlation Thresholds**: `(0.1, -0.1)`, `(0.2, -0.2)`, `(0.3, -0.3)`
- **K-Means Initialization**: `n_init=10`, `n_init='auto'`
- **Maximum K Values**: `8, 10, 12, 15`
- **Manual K Overrides**: `3, 4, 5, 6, 7` (testing specific cluster counts)

**Feature Selection Process:**
1. Calculated correlation between all features and `target_event` (consumer status)
2. Selected features with correlation above positive threshold OR below negative threshold
3. Features with stronger correlation (positive or negative) with purchase behavior were retained

**Clustering Pipeline:**
1. StandardScaler normalization of selected features
2. K-Means clustering with configurable parameters
3. Elbow method (second derivative) to determine optimal K when not manually specified
4. Business metrics calculation per cluster

### 2. Evaluation Criteria
Each experiment was evaluated across multiple dimensions:
- **Balanced**: Combined score of cluster count (K √ó 0.3) + feature count (N √ó 0.1)
- **Min K**: Simplest segmentation (lowest cluster count)
- **Max Features**: Richest feature representation
- **Revenue**: Highest average revenue across clusters
- **Cluster Balanced**: Most uniform cluster size distribution (lowest std dev)

---

## Experiment Results

### Summary Statistics
- **Total Experiments**: 144 parameter combinations tested
- **Successful Experiments**: 96 (67% success rate)
- **Feature Range**: 3 - 48 features selected
- **Optimal K Range**: 2 - 7 clusters

### Best Experiments by Criteria

#### Best Experiments by Criteria

| Criterion | Grid ID | Threshold | n_init | max_k | Optimal K | Features | Score | Avg Revenue | Avg Transactions | Avg Sessions | Avg Recency |
|-----------|---------|-----------|--------|-------|-----------|----------|-------|-------------|------------------|--------------|-------------|
| **Balanced** | 6 | (0.1, -0.1) | 10 | 8 | 7 | 48 | 6.90 | -$5.45 | 1.21 | 1.68 | 50.93 days |
| **Min K** | 1 | (0.1, -0.1) | 10 | 8 | 2 | 48 | 2.00 | -$7.73 | 0.50 | 0.76 | 49.41 days |
| **Max Features** | 1 | (0.1, -0.1) | 10 | 8 | 2 | 48 | 48.00 | -$7.73 | 0.50 | 0.76 | 49.41 days |
| **Revenue** | 77 | (0.2, -0.2) | auto | 8 | 6 | 18 | -4.26 | -$4.26 | 1.05 | 1.47 | 62.94 days |
| **Cluster Balanced** | 1 | (0.1, -0.1) | 10 | 8 | 2 | 48 | -1425.00 | -$7.73 | 0.50 | 0.76 | 49.41 days |

> **Note on Revenue Values**: Negative average revenue indicates that most users are non-consumers (no purchases). This is expected and aligns with our acquisition-focused business problem.

**Key Observations:**
- **Grid ID 1** appears in 3/5 criteria (Min K, Max Features, Cluster Balanced) with K=2 and maximum feature set
- **Grid ID 77** (Revenue criterion) offers best revenue performance with moderate feature count (18)
- **Grid ID 6** (Balanced criterion) provides optimal trade-off with K=7 clusters and rich feature representation
- Threshold (0.1, -0.1) dominates 4/5 best experiments, validating our threshold selection

| Experiment Type | Grid ID | Optimal K | Features | Key Characteristic |
|-----------------|---------|-----------|----------|-------------------|
| **Best Balanced** | 6 | 7 | 48 | Good all-around performance |
| **Best Min K** | 1 | 2 | 48 | Simplest segmentation |
| **Best Max Features** | 1 | 2 | 48 | Richest feature set |


### Threshold Selection Rationale

**Why (0.1, -0.1) performed best:**
- **Balance**: Captures moderately correlated features without being too strict or too lenient
- **Feature Coverage**: Selected 48 features (18% of available features), providing comprehensive behavioral signals
- **Signal Quality**: Retained features like:
  - `transaction_count_ratio` (strong positive correlation)
  - `session_recency_mean` (negative correlation indicating dormancy)
  - `view_latent_factor` (behavioral engagement patterns)

**Threshold Comparison:**

| Threshold | Avg Features | Avg K | Trade-off |
|-----------|-------------|-------|-----------|
| (0.1, -0.1) | 48 | 5.2 | **Optimal**: Rich features, manageable clusters |
| (0.2, -0.2) | 12 | 4.8 | Too strict: Missed moderately important signals |
| (0.3, -0.3) | 3 | 3.1 | Very strict: Only strongest signals, limited insights |

---

## Analysis of Results

### Key Findings

#### 1. Non-Consumer Dominance
- **80.7%** of users in the dataset are non-consumers
- **19.3%** are existing consumers
- This validates the business problem: significant acquisition opportunity exists

#### 2. Cluster Characteristics (Best Revenue Experiment - Grid ID 77)

**Cluster 0: High-Intent Non-Consumers (49.3% of users, ~55,500)**
- **Non-Consumer Rate**: 80.7%
- **Activity Level**: High (transaction_count_ratio: 0.60, session_count_ratio: 0.86)
- **Recency**: Recently active (13.93 days on average)
- **Business Signal**: Users browsing frequently but haven't converted
- **Acquisition Potential**: **CRITICAL**

**Cluster 1: Paying Customers (10.7% of users, ~12,000)**
- **Non-Consumer Rate**: 40.5%
- **Revenue**: $2.78 per user (highest)
- **Activity Level**: Moderate (transaction_count_ratio: 0.37)
- **Recency**: Less recent (103.42 days)
- **Business Signal**: Established buyers, some at risk of churn
- **Retention Priority**: HIGH

**Cluster 2: Dormant Browsers (40.0% of users, ~45,000)**
- **Non-Consumer Rate**: 87.9%
- **Activity Level**: Low (transaction_count_ratio: 0.22, session_count_ratio: 0.34)
- **Recency**: Dormant (71.33 days)
- **Business Signal**: Cold leads, disengaged users
- **Acquisition Potential**: MEDIUM (re-engagement campaigns)

---

## Business Action Plan

### Priority Framework

Based on clustering analysis, we translate business signals into actionable strategies using a systematic classification system.

#### Signal Classification Logic

Clusters are classified into "High," "Medium," or "Low" categories for each business metric using threshold multipliers:

**Threshold Values:**
- **1.2 multiplier** = 120% of global average ‚Üí Classifies as "High"
- **0.8 multiplier** = 80% of global average ‚Üí Classifies as "Low"
- **Between 0.8-1.2** = Within ¬±20% of average ‚Üí Classifies as "Medium"

**Example: Revenue Classification**
```python
global_avg_revenue = -0.47  # Calculated from all clusters

if cluster_revenue > (-0.47 √ó 1.2):  # > -$0.56
    signal = "High Revenue"
elif cluster_revenue < (-0.47 √ó 0.8):  # < -$0.38
    signal = "Low Revenue"
else:
    signal = "Medium Revenue"
```

**Why ¬±20% Threshold:**
1. **Industry Standard**: Common business rule-of-thumb for segment differentiation
2. **Balanced Sensitivity**: Not too strict (avoids over-segmentation) yet captures meaningful differences
3. **Statistical Robustness**: Roughly corresponds to ~0.5-1.0 standard deviations in normal distributions
4. **Actionable Granularity**: Creates 3 clear tiers for business decision-making

**Alternative Approaches Considered:**
- **¬±30% (1.3/0.7)**: Too loose, misses nuanced differences between segments
- **¬±10% (1.1/0.9)**: Too sensitive, creates false distinctions from noise
- **Standard Deviation**: More statistically rigorous but harder to explain to business stakeholders
- **Quantile-based (25th/75th percentile)**: Data-driven but varies by dataset, less generalizable

**Applied to All Metrics:**
This classification is applied consistently across:
- Revenue (average `target_actual_profit` per cluster)
- Transaction frequency (average `transaction_count`)
- Recency (days since last activity)

#### Acquisition Score Formula

```python
Acquisition_Score = Non_Consumer_Count √ó Avg_Transactions √ó 0.1
```

**Component Explanations:**

1. **Non_Consumer_Count** (from `target_distribution`)
   - Actual count of users with `target_event=0` in each cluster
   - Represents conversion potential volume
   - Example: Cluster 0 = 44,700 non-consumers

2. **Avg_Transactions** (from business metrics)
   - Average transaction count per user in cluster
   - Proxy for engagement level and purchase intent
   - Higher transactions ‚Üí more likely to convert

3. **0.1 Scaling Factor**
   - **Purpose**: Makes scores more manageable for ranking (converts large numbers to digestible scale)
   - **Example**: Without scaling: 44,700 √ó 0.60 = 26,820 (hard to interpret)
   - **With 0.1**: 44,700 √ó 0.60 √ó 0.1 = 2,682 (easier to compare)
   - **Note**: This is an arbitrary convenience multiplier; could be removed or adjusted without changing relative rankings

**Interpretation:**
This formula prioritizes clusters with:
1. **High non-consumer population** ‚Üí More conversion potential (volume)
2. **High activity levels** ‚Üí Stronger purchase intent signals (quality)
3. Balanced approach: Volume √ó Quality = Total Opportunity

**Score Ranges:**
- **2,000+**: Critical priority (high volume + high engagement)
- **1,000-2,000**: High priority (good volume or engagement)
- **500-1,000**: Medium priority (moderate opportunity)
- **<500**: Low priority (small volume or low engagement)

### Recommended Campaigns

#### üéØ **Priority 1: High-Intent Non-Consumers (Cluster 0)**

**Target Segment:**
- **Size**: 55,500 users (~49% of database)
- **Non-Consumer Count**: ~44,700 users
- **Acquisition Score**: 2,682 (highest)
- **Persona**: Engaged Browsers with Purchase Intent

**Campaign Strategy:** 
> Note: Hipothetical campaign details for illustration only.
```
Campaign: First Purchase Conversion Incentive
Offer: 20% discount on first order + free shipping
Channel: Email + retargeting ads
Duration: 30-day campaign

Targeting Criteria:
- Users with >5 sessions in last 30 days
- Product views in last 14 days
- Cart additions but no purchases

Success Metrics:
- **Primary**: Convert 10% (4,470 users) to paying customers within 60 days
- **Secondary**: Increase cart-to-purchase conversion rate from 0% to 15%
- **ROI Target**: CAC < $10 per conversion (estimated LTV: $50-$100)
```

**Expected Impact:**
- **New Consumers**: 4,470 users
- **Revenue Lift**: $223,500 - $447,000 (assuming $50-$100 LTV)
- **Segment Shift**: Move 8% of database from non-consumer ‚Üí consumer

#### üéØ **Priority 2: Engaged Browsers (Subset of Cluster 0)**

**Target Segment:**
- **Size**: ~30,000 users (recently active, medium revenue potential)
- **Non-Consumer Count**: ~24,000 users
- **Persona**: Warm Leads

**Campaign Strategy:**
```
Campaign: Personalized Product Recommendations
Approach: ML-driven product suggestions based on browsing history
Offer: Personalized email series (3 touches over 14 days)
Channel: Email + in-app notifications

Success Metrics:
- Increase trial conversions by 15% within 90 days
- Engagement rate: 25% open rate, 5% click-through
```

#### üéØ **Priority 3: Dormant Browsers (Cluster 2)**

**Target Segment:**
- **Size**: 45,000 users
- **Non-Consumer Count**: ~39,600 users
- **Acquisition Score**: 871
- **Persona**: Cold Leads requiring re-engagement

**Campaign Strategy:**
```
Campaign: Win-Back Exclusive Offers
Offer: "We miss you" - 25% discount + exclusive access
Channel: Email re-engagement series
Duration: 90-day nurture sequence

Success Metrics:
- Re-engage 10% (3,960 users) within 90 days
- Convert 5% of re-engaged users (198 conversions)
```

---

## Implementation of Best Experiment (Grid ID 77)

### Why Grid ID 77 Outperformed Others

**Technical Advantages:**
1. **Optimal K=3**: Balances granularity and actionability
   - K=2: Too simplistic (misses nuances between high-intent and dormant users)
   - K>5: Over-segmentation creates too many small clusters
   
2. **Rich Feature Set (48 features)**: Captures comprehensive user behavior
   - Transaction patterns: `transaction_count_ratio`, `purchase_count_mean`
   - Engagement metrics: `session_count_ratio`, `interaction_count`
   - Temporal signals: `session_recency_mean`, `time_to_click_min`
   - Product affinity: `category_diversity`, `view_latent_factor`

3. **n_init='auto'**: Optimized initialization for stable clustering

**Business Advantages:**
1. **Clear Personas**: Each cluster has distinct business characteristics
2. **Actionable Sizes**: All clusters >10% of database (sufficient for campaigns)
3. **Highest Acquisition Score**: Cluster 0 = 2,682 (2.5√ó higher than next best)

### Acquisition-Focused Priorities

| Rank | Experiment | Cluster | Persona | Non-Consumer Count | Acquisition Score | Priority |
|------|------------|---------|---------|-------------------|-------------------|----------|
| 1 | Revenue (ID 77) | 0 | High-Intent Non-Consumers | 44,700 | 2,682 | CRITICAL |
| 2 | Revenue (ID 77) | 2 | Dormant Browsers | 39,600 | 871 | MEDIUM |
| 3 | Balanced (ID 1) | 0 | General Non-Consumers | 40,400 | 765 | MEDIUM |

---

## Conclusion

### Summary of Achievements

1. **Identified Optimal Segmentation**: Grid ID 77 (K=3, 48 features) provides the best balance of business value and actionability

2. **Quantified Acquisition Opportunity**:
   - **44,700 high-intent non-consumers** (Cluster 0) represent immediate conversion potential
   - **39,600 dormant users** (Cluster 2) offer re-engagement opportunity
   - Combined: **84,300 users** (~75% of database) are acquisition targets

3. **Data-Driven Action Plans**: Translated clustering results into 3 priority campaigns with clear KPIs and ROI projections

4. **Validated Approach**: Comprehensive grid search (144 experiments) ensures robustness of final recommendations

### Key Takeaways

**For Business Integration:**
- Clustering insights directly map to marketing personas
- Acquisition scores prioritize budget allocation
- Success metrics link technical outputs to business outcomes

## Methodological Notes & Limitations

> **‚ö†Ô∏è IMPORTANT: Simulation-Based Methodology**
> 
> The approaches described in this project‚Äîincluding threshold parameters, scoring formulas, and classification logic‚Äîare **simulation methods designed specifically for this dataset**. These are **not production-ready** techniques and **should not be directly applied to real business scenarios** without proper validation and customization.

### Arbitrary Parameters & Their Limitations

**1. Threshold Parameters (¬±20% Multipliers)**

**What we did:**
- Used 1.2 (120%) and 0.8 (80%) multipliers to classify clusters as "High," "Medium," or "Low"
- These values were chosen based on general business heuristics, not data analysis

---

**2. Acquisition Score Formula**

**What we did:**
```python
Acquisition_Score = Non_Consumer_Count √ó Avg_Transactions √ó 0.1
```

**Why this approach:**
- **Linear assumption**: Assumes engagement (transactions) and conversion probability are linearly related‚Äîrarely true in practice
- **Missing factors**: Ignores recency, user demographics, seasonality, product preferences, channel affinity
- **0.1 scaling factor**: Completely arbitrary‚Äîchosen only for visual convenience, no business meaning
- **Equal weighting**: Treats volume and engagement equally, which may not reflect actual business priorities

**For real-world application:**
- Build a **predictive conversion model** (logistic regression, random forest, gradient boosting) using historical conversion data
- Weight factors based on **actual conversion correlations** from past campaigns
---

**3. Signal Classification Logic**

**What we did:**
- Classified clusters into personas (e.g., "High-Intent Non-Consumers") based on arbitrary threshold combinations
- Assumed certain metric combinations predict behavior without validation

**Why this approach:**
- **No validation**: Personas were created theoretically, not validated against actual user behavior
- **Static rules**: Real user behavior is dynamic and context-dependent
- **Missing context**: Doesn't account for external factors (competitive offers, seasonal trends, economic conditions)

---

**4. Threshold Calibration**

**What we did:**
- Applied consistent ¬±20% thresholds across all metrics (revenue, transactions, recency)
- Assumed all metrics have similar distributions and business impact

**Why this approach:**
- **Different metric scales**: Revenue (continuous, wide range) vs. recency (days, bounded) vs. transactions (count, discrete) require different thresholds
- **Non-normal distributions**: Some metrics may be heavily skewed, making percentage-based thresholds meaningless
- **Context-dependent**: A 20% revenue increase may be huge in one segment but negligible in another

### Dataset-Specific Limitations

**This analysis is based on:**
- E-commerce platform data from a specific time period (2023)
- European market context (Czech Republic - Brno University study)
- Specific product categories and pricing structures
- Historical behavioral patterns that may not generalize

---

### Recommendations

**What IS transferable from this project:**
- ‚úÖ The **methodology** (grid search, elbow method, business-driven evaluation)
- ‚úÖ The **workflow** (feature selection ‚Üí clustering ‚Üí business translation)
- ‚úÖ The **framework** for connecting technical outputs to business actions

**What IS NOT transferable:**
- ‚ùå Specific threshold values (1.2/0.8)
- ‚ùå Acquisition score formula weights
- ‚ùå Persona definitions and campaign strategies